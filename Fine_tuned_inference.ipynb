{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ey2ZBc8rrG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "    !pip install torch transformers peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. Define model and adapter names\n",
        "base_model_name = \"unsloth/medgemma-4b-pt\"\n",
        "adapter_name = \"huseyincavus/medgemma-4b-guidelines-lora\"\n",
        "\n",
        "# 2. Load the model using Unsloth's FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None, # None will default to torch.bfloat16\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# 3. Load the LoRA adapter directly\n",
        "# This is a cleaner way than using get_peft_model for inference.\n",
        "model.load_adapter(adapter_name)\n",
        "\n",
        "# 4. Use the Unsloth Alpaca prompt template.\n",
        "# This is crucial for getting the correct output format.\n",
        "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "What are the latest guidelines for treating type 2 diabetes?\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# 5. Create a text generation pipeline\n",
        "text_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# 6. Run inference\n",
        "result = text_generator(prompt, max_new_tokens=250, num_return_sequences=1)\n",
        "\n",
        "print(\"--- Model Output ---\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "RBx4jigj83e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The model is already loaded, so we just print the variable\n",
        "print(\"--- Verifying Architecture of In-Memory Model ---\")\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Xp3BcPN9BW9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA Model Integration Evidence\n",
        "\n",
        "The output from printing the model object provides definitive proof that our fine-tuned LoRA adapter has been successfully applied to the base model.\n",
        "\n",
        "The key evidence is the presence of `lora.Linear4bit` wrappers around the model's original layers, such as `q_proj` and `v_proj` in the attention blocks.\n",
        "\n",
        "```python\n",
        "(self_attn): Gemma3Attention(\n",
        "  (q_proj): lora.Linear4bit(\n",
        "    (base_layer): Linear4bit(...)\n",
        "    (lora_A): ModuleDict(...)\n",
        "    (lora_B): ModuleDict(...)\n",
        "  )\n",
        "  ...\n",
        ")\n",
        "```\n",
        "\n",
        "## Architecture Analysis\n",
        "\n",
        "This structure shows that the original layer (`base_layer`) is now augmented with new, trainable LoRA weights (`lora_A` and `lora_B`). These are the weights learned during the fine-tuning process.\n",
        "\n",
        "Their presence confirms that the model is no longer just the base model but a **`PeftModel`** that will use these specialized weights during inference."
      ],
      "metadata": {
        "id": "_2RVUP6UCSaD"
      }
    }
  ]
}
